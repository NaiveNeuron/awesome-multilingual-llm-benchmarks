# awesome-multilingual-llm-benchmarks

A curated list of multilingual and/or non-English benchmarks for Large Language Models (LLMs) or NLP models and tools in general.

## Language-specific Benchmarks

### Foundational Language Understanding Benchmarks

#### Single-Language

*These benchmarks focus on core linguistic competencies for individual languages, testing syntax, semantics, natural language inference, sentiment analysis, named entity recognition, and other fundamental language processing capabilities.*

|Language|Date|Title|Tasks|Links|
|:---:|:---:|:---:|:---:|:---:|
|Arabic 游젏릖뵾2021-04|[ALUE: Arabic Language Understanding Evaluation](https://arxiv.org/abs/2104.01353)|SA, NLI, STS, Dialect ID, Toxicity/Offensive, QA|[[paper]](https://arxiv.org/abs/2104.01353) [[ACL Anthology]](https://aclanthology.org/2021.wanlp-1.18.pdf)|
|Arabic 游젏릖뵾2022-12|[ORCA: A Challenging Benchmark for Arabic Language Understanding](https://arxiv.org/abs/2212.01306)|SA, NLI, QA (MRC/MC), NER, Topic/News Clf, Paraphrase|[[paper]](https://arxiv.org/abs/2212.01306) [[ACL Anthology]](https://aclanthology.org/2023.findings-acl.609/)|
|Basque 游쀯릖젏릖游읖2022-06|[BasqueGLUE: A Natural Language Understanding Benchmark for Basque](https://aclanthology.org/2022.lrec-1.172)|NER, Intent Classification, Slot Filling, Topic Classification, Sentiment Analysis, Stance Detection, QA/NLI, WiC, Coreference Resolution|[[paper]](https://aclanthology.org/2022.lrec-1.172) [[data]](https://huggingface.co/datasets/orai-nlp/basqueGLUE)|
|Bengali 游游뼢2021-01|[BLUB: Bangla Language Understanding Benchmark](https://arxiv.org/abs/2101.00204)|SA, NLI, NER, Span-QA|[[paper]](https://arxiv.org/abs/2101.00204) [[code]](https://github.com/csebuetnlp/banglabert)|
|Belarusian 游游쭆2025-06|[BelarusianGLUE (BelGLUE)](https://arxiv.org/abs/2506.08804)|SA, NLI, QA, NER, Morphology, Topic Classification|[[paper]](https://arxiv.org/abs/2506.08804)|
|Bulgarian 游游샆2023-07|[bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark](https://aclanthology.org/2023.acl-long.487/)|NER, POS Tagging, Sentiment, Check-Worthiness, Humor Detection, NLI, Multi-Choice QA, Factuality Classification|[[paper]](https://aclanthology.org/2023.acl-long.487/) [[code]](https://github.com/bgGLUE/bgglue) [[data]](https://huggingface.co/datasets/bgglue/bgglue)|
|Catalan 游쀯릖잪2021-12|[The Catalan Language CLUB](https://arxiv.org/abs/2112.01894)|NER, POS Tagging, NLI, Document Classification, QA, STS|[[paper]](https://arxiv.org/abs/2112.01894) [[data]](https://huggingface.co/BSC-LT)|
|Chinese 游뻟릖씊2020-04|[CLUE: A Chinese Language Understanding Evaluation Benchmark](https://www.aclweb.org/anthology/2020.coling-main.419)|Short / Long Text Classification, Coreference Resolution, Semantic Similarity, Keyword Recongition, NLI, Machine Reading Comprehension|[[paper]](https://www.aclweb.org/anthology/2020.coling-main.419)
|Danish 游뾇릖쌒2024-05|[Towards a Danish Semantic Reasoning Benchmark](https://aclanthology.org/2024.lrec-main.1421/)|Inference, Entailment, Synonymy, Similarity, Relatedness, Word Sense Disambiguation (WiC) |[[paper]](https://aclanthology.org/2024.lrec-main.1421/)|
|Dutch 游游쎺2023-12|[DUMB: A Benchmark for Smart Evaluation of Dutch Models](https://aclanthology.org/2023.emnlp-main.447/)|POS Tagging, NER, Word Sense Disambiguation, Pronoun Resolution, Causal Reasoning, NLI, Sentiment Analysis, Document Classification, Question Answering|[[paper]](https://aclanthology.org/2023.emnlp-main.447/)|
|Finnish 游游숖2020-10|[Towards Fully Bilingual Deep Language Modeling](https://arxiv.org/abs/2010.11639)|POS Tagging, NER, Dependency Parsing, Document Classification|[[paper]](https://arxiv.org/abs/2010.11639)|
|French 游游읖2020-01|[FLUE: French Language Understanding Evaluation](https://www.fluebenchmark.com/)|Text Classification, Paraphrase, NLI, Parsing, POS, WSD|[[web]](https://www.fluebenchmark.com/) [[paper]](https://arxiv.org/abs/2402.13432)|
|German 游뾇릖뿊2024-06|[SuperGLEBer: German Language Understanding Evaluation Benchmark](https://aclanthology.org/2024.naacl-long.438/)|NER, Document Classification, STS, QA|[[paper]](https://aclanthology.org/2024.naacl-long.438/)|
|Hungarian 游쇓릖죺2024-05|[HuLU: Hungarian Language Understanding Benchmark Kit](https://aclanthology.org/2024.lrec-main.733)|CoPA, RTE, SST, WNLI, CommitmentBank, ReCoRD QA|[[paper]](https://aclanthology.org/2024.lrec-main.733)|
|Indonesian 游쉻릖뼢2020-12|[IndoNLU](https://aclanthology.org/2020.aacl-main.85.pdf)|SA, Aspect-SA, Emotion, POS, NER, NLI, Span-QA/KE|[[paper]](https://aclanthology.org/2020.aacl-main.85.pdf) [[ACL Anthology]](https://aclanthology.org/2020.aacl-main.85/)|
|Indonesian 游쉻릖뼢2020-11|[IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP](https://arxiv.org/abs/2011.00677)|Morpho-syntax (POS), Semantics, Discourse (7 tasks)|[[paper]](https://arxiv.org/abs/2011.00677) [[ACL Anthology]](https://aclanthology.org/2020.coling-main.66/)|
|Italian 游쉻릖졒2023-07|[UINAUIL: A Unified Benchmark for Italian Natural Language Understanding](https://aclanthology.org/2023.acl-demo.33)|Textual Entailment, Event Detection & Classification (EVENTI), Factuality Classification (FactA), Sentiment Analysis (SENTIPOLC), Irony Detection (IronITA), Hate Speech Detection (HaSpeeDe) |[[paper]](https://aclanthology.org/2023.acl-demo.33)|
|Japanese 游游옆2022-07|[JGLUE: Japanese General Language Understanding Evaluation](https://aclanthology.org/2022.findings-acl.80.pdf)|SA (MARC-ja), NLI (JNLI), STS (JSTS), QA (JSQuAD/JCommonsenseQA), Acceptability (JCoLA)|[[paper]](https://aclanthology.org/2022.findings-acl.80.pdf) [[ACL Anthology]](https://aclanthology.org/2022.lrec-1.317/)|
|Norwegian 游游앞2023-05|[NorBench -- A Benchmark for Norwegian Language Models](https://arxiv.org/abs/2305.03880)|Morpho-syntactic tasks (POS Tagging, Lemmatization, Dependency Parsing), NER, Sentiment Analysis (Document-level, Sentence-level, Targeted), Linguistic Acceptability, Question Answering, Machine Translation, Diagnostics of Harmful Predictions (Gender Bias, Harmfulness)|[[paper]](https://arxiv.org/abs/2305.03880) [[code]](https://github.com/ltgoslo/norbench)|
|Persian 游쉻릖읖2020-12|[ParsiNLU: A Suite of Language Understanding Challenges for Persian](https://arxiv.org/abs/2012.06154)|Reading Comprehension, Textual Entailment, Sentiment Analysis, Question Paraphrasing, Machine Translation, Query Paraphrasing|[[paper]](https://arxiv.org/abs/2012.06154) [[ACL Anthology]](https://aclanthology.org/2021.tacl-1.68.pdf)|
|Polish 游왫릖쎺2020-05|[KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://arxiv.org/abs/2005.00630)|NER, Sentence Relatedness, Textual Entailment, Cyberbullying Detection, Sentiment Analysis (In-Domain & Out-of-Domain), Question Answering, Paraphrase Detection, Sentiment Analysis (Allegro Reviews)|[[paper]](https://arxiv.org/abs/2005.00630)|
|Polish 游왫릖쎺2022-12|[This is the way: Designing and Compiling LEPISZCZE, a Comprehensive NLP Benchmark for Polish](https://proceedings.neurips.cc/paper_files/paper/2022/file/890b206ebb79e550f3988cb8db936f42-Paper-Datasets_and_Benchmarks.pdf)|Sentiment Analysis, Abusive Clauses Detection, Political Advertising Detection, NLI, NER, POS Tagging, Paraphrase Classification, Punctuation Restoration, Dialogue Acts Classification |[[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/file/890b206ebb79e550f3988cb8db936f42-Paper-Datasets_and_Benchmarks.pdf)|
|Portuguese 游왫릖좷릖游읖2024-04|[PORTULAN ExtraGLUE Datasets and Models](https://arxiv.org/abs/2404.05333)|SST-2, MRPC, STS-B, MNLI, QNLI, RTE, WNLI, BoolQ, MultiRC, CoPA |[[paper]](https://arxiv.org/abs/2404.05333)|
|Romanian 游游앞2021-12|[LiRo: Benchmark and Leaderboard for Romanian Language Tasks](https://openreview.net/pdf?id=JH61CD7afTv)|Document Classification, NER, Machine Translation, Sentiment Analysis, POS Tagging, Dependency Parsing, Language Modeling, QA, STS, Gender Debiasing|[[paper]](https://openreview.net/pdf?id=JH61CD7afTv) [[web]](https://lirobenchmark.github.io/)|
|Russian 游游죺2020-10|[RussianSuperGLUE](https://arxiv.org/abs/2010.15925)|Commonsense/COPA-like, RTE/NLI, QA, WSC-like, Paraphrase|[[paper]](https://arxiv.org/abs/2010.15925) [[ACL Anthology]](https://www.aclweb.org/anthology/2020.emnlp-main.381.pdf)|
|Slovak 游젏릖쌒2025-01|[skLEP: A Slovak General Language Understanding Benchmark](https://arxiv.org/abs/2506.21508)|Sentiment Analysis, NER, Text Classification, Paraphrase Detection, Word Sense Disambiguation|[[paper]](https://arxiv.org/abs/2506.21508)|
|Slovenian 游젏릖숖2022-02|[Slovene SuperGLUE Benchmark: Translation and Evaluation](https://arxiv.org/abs/2202.04994)|BoolQ, CB, COPA, MultiRC, RTE, WSC|[[paper]](https://arxiv.org/abs/2202.04994)|
|Swedish 游젏릖뿊2023-12|[Superlim: A Swedish Language Understanding Evaluation Benchmark](https://aclanthology.org/2023.emnlp-main.506/)|Absabank-Imm, Argumentation Sentences, DaLAJ-GED, SweParaphrase, SweDN, SweFAQ, SweNLI, SweWiC, SweWinograd, SuperSim, Swedish Analogy, SweSAT, SweDiagnostics, SweWinogender |[[paper]](https://aclanthology.org/2023.emnlp-main.506/)|
|Vietnamese 游游씊2024-06|[ViGLUE: A Vietnamese General Language Understanding Benchmark and Analysis of Vietnamese Language Models](https://aclanthology.org/2024.findings-naacl.261.pdf)|MNLI, QNLI, RTE, VNRTE, WNLI, SST2, VSFC, VSMEC, MRPC, QQP, CoLA, VToC|[[paper]](https://aclanthology.org/2024.findings-naacl.261.pdf) [[code]](https://github.com/trminhnam/ViGLUE) [[data]](https://huggingface.co/datasets/tmnam20/ViGLUE)|
|American Sign Language 游쥟릖잪2025-05|[EmoSign: ASL Emotion Recognition Dataset](https://arxiv.org/abs/2505.17090)|200 ASL videos with sentiment and emotion labels for multimodal emotion recognition|[[paper]](https://arxiv.org/abs/2505.17090)|
|Basque 游쀯릖잪2025-01|[BasqBBQ: A QA Benchmark for Assessing Social Biases in LLMs for Basque](https://aclanthology.org/2025.coling-main.318/)|Social bias assessment in Basque across eight domains|[[paper]](https://aclanthology.org/2025.coling-main.318/)|
|Yoruba Dialects 游游샆2024-06|[YORULECT: Yoruba Regional Dialects Benchmark](https://arxiv.org/abs/2406.19564)|Regional dialect evaluation across four Yoruba language regions|[[paper]](https://arxiv.org/abs/2406.19564)|

#### Multilingual

*These benchmarks cover multiple languages and focus on core linguistic competencies such as syntax, semantics, natural language inference, sentiment analysis, named entity recognition, and other fundamental language processing capabilities across different language families.*

|Languages|Date|Title|Tasks|Links|
|:---:|:---:|:---:|:---:|:---:|
|African Languages 游깴|2023-11|[AfroBench: How Good are Large Language Models on African Languages?](https://arxiv.org/abs/2311.07978)|Sentiment Analysis, Topic Classification, Named Entity Recognition, Question Answering, Language Identification (64 languages, 15 tasks)|[[paper]](https://arxiv.org/abs/2311.07978)|
|African Languages 游깴|2024-06|[Uhura: A Synthetic Multilingual Dataset for Instruction Tuning](https://arxiv.org/abs/2406.11096)|Multiple-choice, open-generation, natural language inference (200+ African languages)|[[paper]](https://arxiv.org/abs/2406.11096)|
|African Languages 游깴|2024-08|[IrokoBench: A New Benchmark for African Languages in the Age of Large Language Models](https://arxiv.org/abs/2408.12388)|Nigerian languages evaluation benchmark (Yoruba, Igbo, Hausa, Pidgin)|[[paper]](https://arxiv.org/abs/2408.12388)|
|Cross-lingual SEA 游깶|2023-09|[BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation Suite (Indonesian, Vietnamese, Thai, Tamil)](https://arxiv.org/abs/2309.06085)|Sentiment Analysis, Named Entity Recognition, Natural Language Inference, Part-of-Speech Tagging, Dependency Parsing|[[paper]](https://arxiv.org/abs/2309.06085)|
|Indic Languages 游쉻릖씊2024-11|[MILU: A Multi-task Indic Language Understanding Benchmark](https://arxiv.org/abs/2411.02538)|Text Classification, Natural Language Inference, Named Entity Recognition, Part-of-Speech Tagging, Sentiment Analysis|[[paper]](https://arxiv.org/abs/2411.02538)|
|Turkic 游깷|2024-03|[Karde-NLU (Azeri, Kazakh, Kyrgyz, Uzbek, Uyghur)](https://aclanthology.org/2024.eacl-long.100.pdf)|NLI, STS, COPA|[[paper]](https://aclanthology.org/2024.eacl-long.100.pdf) [[data]](https://github.com/lksenel/Kardes-NLU)|
|Nordic/Scandinavian 游깷|2024-02|[ScandEval: Evaluating Large Language Models on Scandinavian Languages](https://arxiv.org/abs/2311.00490)|Sentiment analysis, linguistic acceptability, NER, QA (Danish, Swedish, Norwegian, Icelandic, Faroese)|[[paper]](https://arxiv.org/abs/2311.00490)|
|Indigenous Americas 游깵|2022-12|[AmericasNLI: Machine translation and natural language inference systems for Indigenous languages of the Americas](https://www.frontiersin.org/articles/10.3389/frai.2022.995667/full)|Natural language inference (Ash치ninka, Aymara, Bribri, Guarani, Nahuatl, Otom칤, Quechua, Rar치muri, Shipibo-Konibo, Wixarika)|[[paper]](https://www.frontiersin.org/articles/10.3389/frai.2022.995667/full)|
|Language Varieties (281) 游깷|2024-03|[DIALECTBENCH: A Large-scale Benchmark for 281 Language Varieties](https://arxiv.org/abs/2403.11009)|Comprehensive benchmark covering 281 language varieties and dialects worldwide|[[paper]](https://arxiv.org/abs/2403.11009)|

### Holistic Benchmarks

*These benchmarks focus on factual knowledge, domain expertise, curriculum-based assessments, and subject-matter competency. They test models' ability to recall and apply knowledge from specific academic domains, cultural contexts, or educational curricula.*

#### Single-Language

|Language|Date|Title|Tasks|Links|
|:---:|:---:|:---:|:---:|:---:|
|Ancient Chinese 游뻟릖씊2024-05|[ACLUE: Ancient Chinese Language Understanding Evaluation](https://arxiv.org/abs/2310.09550)|15 tasks spanning phonetic, lexical, syntactic, semantic, inference and knowledge (Xia dynasty to Ming dynasty)|[[paper]](https://arxiv.org/abs/2310.09550)|
|Ancient Chinese 游뻟릖씊2025-03|[F칯x칣: Ancient Chinese Text Understanding and Generation Benchmark](https://arxiv.org/abs/2503.15837)|21 diverse tasks evaluating both understanding and generation for ancient Chinese|[[paper]](https://arxiv.org/abs/2503.15837)|
|Arabic 游젏릖뵾2024-02|[ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic](https://arxiv.org/abs/2402.12840)|Multichoice QA across diverse subjects from school exams in North Africa, Levant, and Gulf regions|[[paper]](https://arxiv.org/abs/2402.12840)|
|Armenian 游뷣릖쑢2024-05|[Exploring the Linguistic Efficiency of Large Language Models in Armenian Discourse](https://cse.aua.am/files/2024/05/Exploring-the-Linguistic-Efficiency-of-Large-Language-Models-in-Armenian-Discourse.pdf)|Armenian language understanding and generation evaluation|[[paper]](https://cse.aua.am/files/2024/05/Exploring-the-Linguistic-Efficiency-of-Large-Language-Models-in-Armenian-Discourse.pdf)|
|Azerbaijani 游뷣릖쯮2024-06|[Open foundation models for Azerbaijani language](https://aclanthology.org/2024.sigturk-1.2/)|Azerbaijani language model evaluation and benchmarking|[[paper]](https://aclanthology.org/2024.sigturk-1.2/)|
|Bengali 游游뼢2025-05|[BnMMLU: Measuring Massive Multitask Language Understanding in Bengali](https://arxiv.org/abs/2505.18951)|Multiple-choice factual knowledge and problem-solving assessment|[[paper]](https://arxiv.org/abs/2505.18951)|
|Cantonese 游쇓릖썷릖뻟릖씊2024-08|[How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models](https://arxiv.org/abs/2408.16756)|Yue-TruthfulQA, Yue-GSM8K, Yue-ARC-C, Yue-MMLU, Yue-TRANS|[[paper]](https://arxiv.org/abs/2408.16756)|
|Chinese 游뻟릖씊2023-06|[CMMLU: Measuring massive multitask language understanding in Chinese](https://arxiv.org/abs/2306.09212)|Multichoice QA across diverse subjects equivalent to MMLU for Chinese|[[paper]](https://arxiv.org/abs/2306.09212)|
|Chinese 游뻟릖씊2024-01|[CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark](https://arxiv.org/abs/2401.11944)|College-level multimodal understanding tasks requiring Chinese cultural context|[[paper]](https://arxiv.org/abs/2401.11944)|
|Chinese 游뻟릖씊2024-09|[CJEval: A Benchmark for Assessing Large Language Models Using Chinese Junior High School Exam Data](https://arxiv.org/abs/2409.16202)|Multi-Choice QA, Bool QA, Fill-in-the Blank QA, Analysis QA|[[paper]](https://arxiv.org/abs/2409.16202)|
|Chinese 游뻟릖씊2024-03|[LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models](https://arxiv.org/abs/2403.12601)|Multi-subject knowledge evaluation with objective and subjective questions|[[paper]](https://arxiv.org/abs/2403.12601)|
|Classical Chinese 游뻟릖씊2024-05|[C3Bench: Comprehensive Classical Chinese Understanding Benchmark](https://arxiv.org/abs/2405.17732)|50,000 text pairs for classification, retrieval, NER, punctuation, translation across 10 domains|[[paper]](https://arxiv.org/abs/2405.17732)|
|Czech 游뻟릖쯮2024-04|[Czech Evaluation of Large Language Models](https://arxiv.org/abs/2405.02428)|Comprehensive evaluation suite for Czech language models|[[paper]](https://arxiv.org/abs/2405.02428)|
|Finnish 游游숖2024-03|[FinnBench: Benchmarking Finnish Natural Language Understanding](https://aclanthology.org/2024.lrec-main.925)|Reading comprehension, sentiment analysis, NLI, word similarity|[[paper]](https://aclanthology.org/2024.lrec-main.925)|
|French 游游읖2024-02|[DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain](https://arxiv.org/abs/2402.13432)|Biomedical language understanding evaluation for French|[[paper]](https://arxiv.org/abs/2402.13432)|
|Hebrew 游쉻릖쎺2024-06|[HeSum: Hebrew Abstractive Summarization and Evaluation](https://arxiv.org/abs/2406.11100)|Hebrew summarization benchmark with evaluation metrics|[[paper]](https://arxiv.org/abs/2406.11100)|
|Hindi Analogy 游쉻릖씊2025-07|[HATS: Hindi Analogy Test Set from Indian Government Exams](https://arxiv.org/abs/2507.13238)|405 multiple-choice analogical reasoning questions from authentic Indian government exams|[[paper]](https://arxiv.org/abs/2507.13238)|
|Hong Kong (Cantonese/Chinese) 游쇓릖쌒2025-05|[Measuring Hong Kong Massive Multi-Task Language Understanding](https://arxiv.org/abs/2505.02177)|Hong Kong linguistic competence and socio-cultural knowledge evaluation|[[paper]](https://arxiv.org/abs/2505.02177)|
|Indic Languages (9) 游쉻릖씊2025-01|[IndicMMLU-Pro: Benchmarking Indic Large Language Models](https://arxiv.org/abs/2501.15747)|Advanced evaluation across Hindi, Bengali, Telugu, Marathi, Tamil, Gujarati, Urdu, Kannada, Punjabi|[[paper]](https://arxiv.org/abs/2501.15747)|
|Italian 游쉻릖졒2024-06|[The Invalsi Benchmarks: Measuring Linguistic and Mathematical Understanding of Large Language Models in Italian](https://arxiv.org/abs/2406.17535)|Locate and Identify Information, Reconstruct Meaning, Reflect on Content/Form, Word Formation, Lexicon and Semantics, Morphology, Spelling, Syntax, Textuality and Pragmatics, Cloze (Fill-in-the-Blank), Multiple Choice (MC), Multiple Complex Choice (MCC), Unique Response (RU), Short Response (RB)|[[paper]](https://arxiv.org/abs/2406.17535)|
|Japanese 游游옆2024-10|[JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation](https://arxiv.org/abs/2410.17250)|Expert-level multimodal understanding tasks based on Japanese cultural context|[[paper]](https://arxiv.org/abs/2410.17250)|
|Kazakh 游썷릖쯮2025-02|[KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional Knowledge](https://arxiv.org/abs/2502.12829)|23,000 questions covering STEM, humanities, social sciences with bilingual Kazakh-Russian context|[[paper]](https://arxiv.org/abs/2502.12829)|
|Korean 游썷릖읖2024-06|[KMMLU: Measuring Massive Multitask Language Understanding in Korean](https://arxiv.org/abs/2402.11548)|Multichoice QA across 45 subjects, including STEM, Humanities, Applied Sciences|[[paper]](https://arxiv.org/abs/2402.11548)|
|Latvian/Giriama 游쐟릖游썷릖뿊2025-03|[LAG-MMLU: Benchmarking Frontier LLM Understanding in Latvian and Giriama](https://arxiv.org/abs/2503.11911)|Culturally relevant MMLU subset for Latvian and Giriama languages|[[paper]](https://arxiv.org/abs/2503.11911)|
|Malaysian/Malay 游쓇릖쭆2024-01|[Large Malaysian Language Model Based on Mistral for Enhanced Local Language Understanding](https://arxiv.org/abs/2401.13565)|Tatabahasa (Malay grammar) evaluation and language understanding|[[paper]](https://arxiv.org/abs/2401.13565)|
|Multilingual Financial (5) 游깷|2025-06|[MultiFinBen: Multilingual Multimodal Financial Benchmark](https://arxiv.org/abs/2506.14028)|First multilingual financial benchmark (English, Chinese, Japanese, Spanish, Greek) across modalities|[[paper]](https://arxiv.org/abs/2506.14028)|
|Multilingual Medical 游깷|2024-04|[MedExpQA: Multilingual Medical Question Answering with Explanations](https://arxiv.org/abs/2404.05590)|First multilingual medical benchmark with reference gold explanations written by medical doctors|[[paper]](https://arxiv.org/abs/2404.05590)|
|Multilingual Medical 游깷|2025-01|[Multi-OphthaLingua: Multilingual Ophthalmological QA Benchmark](https://aaai.org/papers/2025/35053)|Multilingual ophthalmological benchmark for low and middle-income countries|[[paper]](https://aaai.org/papers/2025/35053)|
|Russian 游游죺2024-01|[MERA: A Comprehensive LLM Evaluation in Russian](https://arxiv.org/abs/2401.04531)|MathLogicQA, MultiQ, PARus, RCB, ruModAr, ruMultiAr, ruOpenBookQA, ruTiE, ruWorldTree, RWSD, SimpleAr, BPS, CheGeKa, LCS, ruHumanEval, ruMMLU, USE, ruDetox, ruEthics, ruHateSpeech, ruHHH|[[paper]](https://arxiv.org/abs/2401.04531) [[web]](https://mera.a-ai.ru/en)|
|Sanskrit 游쉻릖씊2024-09|[ByT5-Sanskrit: A Unified Model for Sanskrit NLP Tasks](https://arxiv.org/abs/2409.13920)|Unified Sanskrit benchmark for word segmentation, lemmatization, morphosyntactic tagging|[[paper]](https://arxiv.org/abs/2409.13920)|
|Swahili 游썷릖쀯릖좷릖쯮2024-07|[SwahBERT: Language Model of Swahili](https://arxiv.org/abs/2407.11468)|Swahili NLP benchmark with multiple evaluation tasks|[[paper]](https://arxiv.org/abs/2407.11468)|
|Turkish 游좷릖읖2024-12|[TR-MMLU: Setting Standards in Turkish NLP for Large Language Model Evaluation](https://arxiv.org/abs/2501.00593)|Turkish adaptation of MMLU covering diverse academic subjects|[[paper]](https://arxiv.org/abs/2501.00593)|
|Yoruba 游游샆2024-05|[Yor칯b치LLM: A Large Language Model for Yor칯b치 Language](https://arxiv.org/abs/2405.04418)|Yoruba language understanding benchmark with cultural context|[[paper]](https://arxiv.org/abs/2405.04418)|

#### Multilingual

|Languages|Date|Title|Tasks|Links|
|:---:|:---:|:---:|:---:|:---:|
|29 Languages 游깷|2025-03|[MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation](https://arxiv.org/abs/2503.10497)|Comprehensive multitask language understanding across EN, ZH, JA, KO, FR, DE, ES, PT, AR, TH, HI, BN, SW, and 16 other languages|[[paper]](https://arxiv.org/abs/2503.10497) [[web]](https://mmluprox.github.io/)|
|17 Languages 游깷|2025-02|[BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models](https://arxiv.org/abs/2502.07346)|Simple understanding tasks, instruction following, reasoning, long context understanding, code generation|[[paper]](https://arxiv.org/abs/2502.07346)|
|SEA Languages 游깶|2025-02|[SEA-HELM: Southeast Asian Holistic Evaluation of Language Models](https://arxiv.org/abs/2502.14301)|NLP Classics, LLM-specifics, SEA Linguistics, SEA Culture, Safety (Filipino, Indonesian, Tamil, Thai, Vietnamese)|[[paper]](https://arxiv.org/abs/2502.14301)|
|SEA Languages 游깶|2025-02|[SeaExam and SeaBench: Benchmarking LLMs with Local Multilingual Questions in Southeast Asia](https://arxiv.org/abs/2502.06298)|STEM, Humanities, Social Sciences, Other subjects (Indonesian, Thai, Vietnamese)|[[paper]](https://arxiv.org/abs/2502.06298)|
|Global/42 Languages 游깷|2024-12|[Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation](https://arxiv.org/abs/2412.03304)|Improved MMLU across 42 languages with cultural bias mitigation|[[paper]](https://arxiv.org/abs/2412.03304)|
|Turkic Languages 游깷|2025-02|[TUMLU: A Unified and Native Language Understanding Benchmark for Turkic Languages](https://arxiv.org/abs/2502.11020)|Comprehensive multilingual benchmark for Turkic language family|[[paper]](https://arxiv.org/abs/2502.11020)|
|EU20 游쀯릖죺2024-05|[EU20-MMLU: A Benchmark for Evaluating Large Language Models in European Languages](https://aclanthology.org/2024.naacl-industry.11)|Multilingual evaluation across 20 European languages|[[paper]](https://aclanthology.org/2024.naacl-industry.11)|
|Celtic/British Isles 游섫릖游쉻릖뿊2024-?|[BritEval: British Isles Languages Benchmark](https://llm.org.uk/)|Scots, Irish, Welsh, Scottish Gaelic evaluation across multiple domains|[[web]](https://llm.org.uk/)|
|Iberian Languages 游쀯릖젏릖왫릖졒2025-04|[IberBench: LLM Evaluation on Iberian Languages](https://arxiv.org/abs/2504.16921)|101 datasets across 22 task types (Spanish, Portuguese, Catalan, Basque, Galician)|[[paper]](https://arxiv.org/abs/2504.16921)|
|Multimodal 11 Languages 游깷|2024-03|[EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark](https://arxiv.org/abs/2403.10378)|20,932 multiple-choice questions across 11 languages from 7 language families with images, tables, diagrams|[[paper]](https://arxiv.org/abs/2403.10378)|
|39 Languages Multimodal 游깷|2024-10|[PangeaBench: Multilingual Multimodal Benchmark for 39 Languages](https://arxiv.org/abs/2410.16153)|Holistic evaluation spanning 14 datasets in 47 languages with multimodal capabilities|[[paper]](https://arxiv.org/abs/2410.16153)|
|Sign Languages 游릎2024-08|[FLEURS-ASL: Including American Sign Language in Massively Multilingual Evaluation](https://arxiv.org/abs/2408.13585)|American Sign Language extension of FLORES benchmark for multimodal evaluation|[[paper]](https://arxiv.org/abs/2408.13585)|
|Code-Switching 10 Languages 游깷|2024-06|[Code-Switching Red-Teaming: LLM Safety and Multilingual Understanding](https://arxiv.org/abs/2406.15481)|Safety evaluation with code-switching queries combining up to 10 languages|[[paper]](https://arxiv.org/abs/2406.15481)|
