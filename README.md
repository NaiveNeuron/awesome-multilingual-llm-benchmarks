# awesome-multilingual-llm-benchmarks
A curated list of multilingual and/or non-English benchmarks for Large Language Models (LLMs) or NLP models and tools in general.


### Language-specific Benchmarks

|Language|Date|Title|Tasks|Links|
|:---:|:---:|:---:|:---:|:---:|
|Basque ðŸ‡ªðŸ‡¸ðŸ‡«ðŸ‡·|2022-06|[BasqueGLUE: A Natural Language Understanding Benchmark for Basque](https://aclanthology.org/2022.lrec-1.172)|NER, Intent Classification, Slot Filling, Topic Classification, Sentiment Analysis, Stance Detection, QA/NLI, WiC, Coreference Resolution|[[paper]](https://aclanthology.org/2022.lrec-1.172) [[data]](https://huggingface.co/datasets/orai-nlp/basqueGLUE)|
|Bulgarian ðŸ‡§ðŸ‡¬|2023-07|[bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark](https://aclanthology.org/2023.acl-long.487/)|NER, POS Tagging, Sentiment, Check-Worthiness, Humor Detection, NLI, Multi-Choice QA, Factuality Classification|[[paper]](https://aclanthology.org/2023.acl-long.487/) [[code]](https://github.com/bgGLUE/bgglue) [[data]](https://huggingface.co/datasets/bgglue/bgglue)|
|Cantonese ðŸ‡­ðŸ‡°ðŸ‡¨ðŸ‡³|2024-08|[How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models](https://arxiv.org/abs/2408.16756)||[[paper]](https://arxiv.org/abs/2408.16756)|
|Catalan ðŸ‡ªðŸ‡¸|2021-12|[The Catalan Language CLUB](https://arxiv.org/abs/2112.01894)|NER, POS Tagging, NLI, Document Classification, QA, STS|[[paper]](https://arxiv.org/abs/2112.01894) [[data]](https://huggingface.co/BSC-LT)|
|Chinese ðŸ‡¨ðŸ‡³|2024-09|[CJEval: A Benchmark for Assessing Large Language Models Using Chinese Junior High School Exam Data](https://arxiv.org/abs/2409.16202)|Multi-Choice QA, Bool QA, Fill-in-the Blank QA, Analysis QA|[[paper]](https://arxiv.org/abs/2409.16202)|
|Chinese ðŸ‡¨ðŸ‡³|2020-04|[CLUE: A Chinese Language Understanding Evaluation Benchmark](https://www.aclweb.org/anthology/2020.coling-main.419.pdf)||[[paper]](https://www.aclweb.org/anthology/2020.coling-main.419.pdf)|
|Danish ðŸ‡©ðŸ‡°|2024-05|[Towards a Danish Semantic Reasoning Benchmark](https://aclanthology.org/2024.lrec-main.1421/)||[[paper]](https://aclanthology.org/2024.lrec-main.1421/)|
|Dutch ðŸ‡³ðŸ‡±|2023-12|[DUMB: A Benchmark for Smart Evaluation of Dutch Models](https://aclanthology.org/2023.emnlp-main.447/)||[[paper]](https://aclanthology.org/2023.emnlp-main.447/)|
|Finnish ðŸ‡«ðŸ‡®|2020-10|[Towards Fully Bilingual Deep Language Modeling](https://arxiv.org/abs/2010.11639)|POS Tagging, NER, Dependency Parsing, Document Classification|[[paper]](https://arxiv.org/abs/2010.11639)|
|German ðŸ‡©ðŸ‡ª|2024-06|[SuperGLEBer: German Language Understanding Evaluation Benchmark](https://aclanthology.org/2024.naacl-long.438/)|NER, Document Classification, STS, QA|[[paper]](https://aclanthology.org/2024.naacl-long.438/)|
|Hungarian ðŸ‡­ðŸ‡º|2024-05|[HuLU: Hungarian Language Understanding Benchmark Kit](https://aclanthology.org/2024.lrec-main.733)|CoPA, RTE, SST, WNLI, CommitmentBank, ReCoRD QA|[[paper]](https://aclanthology.org/2024.lrec-main.733)|
|Italian ðŸ‡®ðŸ‡¹|2023-07|[UINAUIL: A Unified Benchmark for Italian Natural Language Understanding](https://aclanthology.org/2023.acl-demo.33)||[[paper]](https://aclanthology.org/2023.acl-demo.33)|
|Italian ðŸ‡®ðŸ‡¹|2024-06|[The Invalsi Benchmarks: Measuring Linguistic and Mathematical Understanding of Large Language Models in Italian](https://arxiv.org/abs/2406.17535)||[[paper]](https://arxiv.org/abs/2406.17535)|
|Korean ðŸ‡°ðŸ‡·|2024-06|[KMMLU: Measuring Massive Multitask Language Understanding in Korean](https://arxiv.org/abs/2402.11548)||[[paper]](https://arxiv.org/abs/2402.11548)|
|Norwegian ðŸ‡³ðŸ‡´|2023-05|[NorBench -- A Benchmark for Norwegian Language Models](https://arxiv.org/abs/2305.03880)||[[paper]](https://arxiv.org/abs/2305.03880)|
|Polish ðŸ‡µðŸ‡±|2020-05|[KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://arxiv.org/abs/2005.00630)||[[paper]](https://arxiv.org/abs/2005.00630)|
|Polish ðŸ‡µðŸ‡±|2022-12|[This is the way: Designing and Compiling LEPISZCZE, a Comprehensive NLP Benchmark for Polish](https://proceedings.neurips.cc/paper_files/paper/2022/file/890b206ebb79e550f3988cb8db936f42-Paper-Datasets_and_Benchmarks.pdf)||[[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/file/890b206ebb79e550f3988cb8db936f42-Paper-Datasets_and_Benchmarks.pdf)|
|Portuguese ðŸ‡µðŸ‡¹ðŸ‡§ðŸ‡·|2024-04|[PORTULAN ExtraGLUE Datasets and Models](https://arxiv.org/abs/2404.05333)||[[paper]](https://arxiv.org/abs/2404.05333)|
|Romanian ðŸ‡·ðŸ‡´|2021-12|[LiRo: Benchmark and Leaderboard for Romanian Language Tasks](https://openreview.net/pdf?id=JH61CD7afTv)||[[paper]](https://openreview.net/pdf?id=JH61CD7afTv) [[web]](https://lirobenchmark.github.io/)|
|Russian ðŸ‡·ðŸ‡º|2024-01|[MERA: A Comprehensive LLM Evaluation in Russian](https://arxiv.org/abs/2401.04531)||[[paper]](https://arxiv.org/abs/2401.04531) [[web]](https://mera.a-ai.ru/en)|
|Slovenian ðŸ‡¸ðŸ‡®|2022-02|[Slovene SuperGLUE Benchmark: Translation and Evaluation](https://arxiv.org/abs/2202.04994)||[[paper]](https://arxiv.org/abs/2202.04994)|
|Swedish ðŸ‡¸ðŸ‡ª|2023-12|[Superlim: A Swedish Language Understanding Evaluation Benchmark](https://aclanthology.org/2023.emnlp-main.506/)||[[paper]](https://aclanthology.org/2023.emnlp-main.506/)|
