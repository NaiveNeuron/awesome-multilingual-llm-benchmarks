# awesome-multilingual-llm-benchmarks

A curated list of multilingual and/or non-English benchmarks for Large Language Models (LLMs) or NLP models and tools in general.

## Language-specific Benchmarks

### Multilingual Foundational Language Understanding Benchmarks

*These benchmarks cover multiple languages and focus on core linguistic competencies such as syntax, semantics, natural language inference, sentiment analysis, named entity recognition, and other fundamental language processing capabilities across different language families.*

|Languages|Date|Title|Tasks|Links|
|:---:|:---:|:---:|:---:|:---:|
|African Languages ğŸŒ|2023-11|[AfroBench: How Good are Large Language Models on African Languages?](https://arxiv.org/abs/2311.07978)|Sentiment Analysis, Topic Classification, Named Entity Recognition, Question Answering, Language Identification (64 languages, 15 tasks)|[[paper]](https://arxiv.org/abs/2311.07978)|
|Cross-lingual SEA ğŸŒ|2023-09|[BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation Suite (Indonesian, Vietnamese, Thai, Tamil)](https://arxiv.org/abs/2309.06085)|Sentiment Analysis, Named Entity Recognition, Natural Language Inference, Part-of-Speech Tagging, Dependency Parsing|[[paper]](https://arxiv.org/abs/2309.06085)|
|Indic Languages ğŸ‡®ğŸ‡³|2024-11|[MILU: A Multi-task Indic Language Understanding Benchmark](https://arxiv.org/abs/2411.02538)|Text Classification, Natural Language Inference, Named Entity Recognition, Part-of-Speech Tagging, Sentiment Analysis|[[paper]](https://arxiv.org/abs/2411.02538)|
|Turkic ğŸŒ|2024-03|[KardeÅŸ-NLU (Azeri, Kazakh, Kyrgyz, Uzbek, Uyghur)](https://aclanthology.org/2024.eacl-long.100.pdf)|NLI, STS, COPA|[[paper]](https://aclanthology.org/2024.eacl-long.100.pdf) [[data]](https://github.com/lksenel/Kardes-NLU)|

### Single-Language Foundational Language Understanding Benchmarks  

*These benchmarks focus on core linguistic competencies for individual languages, testing syntax, semantics, natural language inference, sentiment analysis, named entity recognition, and other fundamental language processing capabilities.*

|Language|Date|Title|Tasks|Links|
|:---:|:---:|:---:|:---:|:---:|
|Arabic ğŸ‡¸ğŸ‡¦|2021-04|[ALUE: Arabic Language Understanding Evaluation](https://arxiv.org/abs/2104.01353)|SA, NLI, STS, Dialect ID, Toxicity/Offensive, QA|[[paper]](https://arxiv.org/abs/2104.01353) [[ACL Anthology]](https://aclanthology.org/2021.wanlp-1.18.pdf)|
|Arabic ğŸ‡¸ğŸ‡¦|2022-12|[ORCA: A Challenging Benchmark for Arabic Language Understanding](https://arxiv.org/abs/2212.01306)|SA, NLI, QA (MRC/MC), NER, Topic/News Clf, Paraphrase|[[paper]](https://arxiv.org/abs/2212.01306) [[ACL Anthology]](https://aclanthology.org/2023.findings-acl.609/)|
|Basque ğŸ‡ªğŸ‡¸ğŸ‡«ğŸ‡·|2022-06|[BasqueGLUE: A Natural Language Understanding Benchmark for Basque](https://aclanthology.org/2022.lrec-1.172)|NER, Intent Classification, Slot Filling, Topic Classification, Sentiment Analysis, Stance Detection, QA/NLI, WiC, Coreference Resolution|[[paper]](https://aclanthology.org/2022.lrec-1.172) [[data]](https://huggingface.co/datasets/orai-nlp/basqueGLUE)|
|Bengali ğŸ‡§ğŸ‡©|2021-01|[BLUB: Bangla Language Understanding Benchmark](https://arxiv.org/abs/2101.00204)|SA, NLI, NER, Span-QA|[[paper]](https://arxiv.org/abs/2101.00204) [[code]](https://github.com/csebuetnlp/banglabert)|
|Belarusian ğŸ‡§ğŸ‡¾|2025-06|[BelarusianGLUE (BelGLUE)](https://arxiv.org/abs/2506.08804)|SA, NLI, QA, NER, Morphology, Topic Classification|[[paper]](https://arxiv.org/abs/2506.08804)|
|Bulgarian ğŸ‡§ğŸ‡¬|2023-07|[bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark](https://aclanthology.org/2023.acl-long.487/)|NER, POS Tagging, Sentiment, Check-Worthiness, Humor Detection, NLI, Multi-Choice QA, Factuality Classification|[[paper]](https://aclanthology.org/2023.acl-long.487/) [[code]](https://github.com/bgGLUE/bgglue) [[data]](https://huggingface.co/datasets/bgglue/bgglue)|
|Catalan ğŸ‡ªğŸ‡¸|2021-12|[The Catalan Language CLUB](https://arxiv.org/abs/2112.01894)|NER, POS Tagging, NLI, Document Classification, QA, STS|[[paper]](https://arxiv.org/abs/2112.01894) [[data]](https://huggingface.co/BSC-LT)|
|Chinese ğŸ‡¨ğŸ‡³|2020-04|[CLUE: A Chinese Language Understanding Evaluation Benchmark](https://www.aclweb.org/anthology/2020.coling-main.419)|Short / Long Text Classification, Coreference Resolution, Semantic Similarity, Keyword Recongition, NLI, Machine Reading Comprehension|[[paper]](https://www.aclweb.org/anthology/2020.coling-main.419)
|Danish ğŸ‡©ğŸ‡°|2024-05|[Towards a Danish Semantic Reasoning Benchmark](https://aclanthology.org/2024.lrec-main.1421/)|Inference, Entailment, Synonymy, Similarity, Relatedness, Word Sense Disambiguation (WiC) |[[paper]](https://aclanthology.org/2024.lrec-main.1421/)|
|Dutch ğŸ‡³ğŸ‡±|2023-12|[DUMB: A Benchmark for Smart Evaluation of Dutch Models](https://aclanthology.org/2023.emnlp-main.447/)|POS Tagging, NER, Word Sense Disambiguation, Pronoun Resolution, Causal Reasoning, NLI, Sentiment Analysis, Document Classification, Question Answering|[[paper]](https://aclanthology.org/2023.emnlp-main.447/)|
|Finnish ğŸ‡«ğŸ‡®|2020-10|[Towards Fully Bilingual Deep Language Modeling](https://arxiv.org/abs/2010.11639)|POS Tagging, NER, Dependency Parsing, Document Classification|[[paper]](https://arxiv.org/abs/2010.11639)|
|French ğŸ‡«ğŸ‡·|2020-01|[FLUE: French Language Understanding Evaluation](https://www.fluebenchmark.com/)|Text Classification, Paraphrase, NLI, Parsing, POS, WSD|[[web]](https://www.fluebenchmark.com/) [[paper]](https://arxiv.org/abs/2402.13432)|
|German ğŸ‡©ğŸ‡ª|2024-06|[SuperGLEBer: German Language Understanding Evaluation Benchmark](https://aclanthology.org/2024.naacl-long.438/)|NER, Document Classification, STS, QA|[[paper]](https://aclanthology.org/2024.naacl-long.438/)|
|Hungarian ğŸ‡­ğŸ‡º|2024-05|[HuLU: Hungarian Language Understanding Benchmark Kit](https://aclanthology.org/2024.lrec-main.733)|CoPA, RTE, SST, WNLI, CommitmentBank, ReCoRD QA|[[paper]](https://aclanthology.org/2024.lrec-main.733)|
|Indonesian ğŸ‡®ğŸ‡©|2020-12|[IndoNLU](https://aclanthology.org/2020.aacl-main.85.pdf)|SA, Aspect-SA, Emotion, POS, NER, NLI, Span-QA/KE|[[paper]](https://aclanthology.org/2020.aacl-main.85.pdf) [[ACL Anthology]](https://aclanthology.org/2020.aacl-main.85/)|
|Indonesian ğŸ‡®ğŸ‡©|2020-11|[IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP](https://arxiv.org/abs/2011.00677)|Morpho-syntax (POS), Semantics, Discourse (7 tasks)|[[paper]](https://arxiv.org/abs/2011.00677) [[ACL Anthology]](https://aclanthology.org/2020.coling-main.66/)|
|Italian ğŸ‡®ğŸ‡¹|2023-07|[UINAUIL: A Unified Benchmark for Italian Natural Language Understanding](https://aclanthology.org/2023.acl-demo.33)|Textual Entailment, Event Detection & Classification (EVENTI), Factuality Classification (FactA), Sentiment Analysis (SENTIPOLC), Irony Detection (IronITA), Hate Speech Detection (HaSpeeDe) |[[paper]](https://aclanthology.org/2023.acl-demo.33)|
|Japanese ğŸ‡¯ğŸ‡µ|2022-07|[JGLUE: Japanese General Language Understanding Evaluation](https://aclanthology.org/2022.findings-acl.80.pdf)|SA (MARC-ja), NLI (JNLI), STS (JSTS), QA (JSQuAD/JCommonsenseQA), Acceptability (JCoLA)|[[paper]](https://aclanthology.org/2022.findings-acl.80.pdf) [[ACL Anthology]](https://aclanthology.org/2022.lrec-1.317/)|
|Norwegian ğŸ‡³ğŸ‡´|2023-05|[NorBench -- A Benchmark for Norwegian Language Models](https://arxiv.org/abs/2305.03880)|Morpho-syntactic tasks (POS Tagging, Lemmatization, Dependency Parsing), NER, Sentiment Analysis (Document-level, Sentence-level, Targeted), Linguistic Acceptability, Question Answering, Machine Translation, Diagnostics of Harmful Predictions (Gender Bias, Harmfulness)|[[paper]](https://arxiv.org/abs/2305.03880) [[code]](https://github.com/ltgoslo/norbench)|
|Persian ğŸ‡®ğŸ‡·|2020-12|[ParsiNLU: A Suite of Language Understanding Challenges for Persian](https://arxiv.org/abs/2012.06154)|Reading Comprehension, Textual Entailment, Sentiment Analysis, Question Paraphrasing, Machine Translation, Query Paraphrasing|[[paper]](https://arxiv.org/abs/2012.06154) [[ACL Anthology]](https://aclanthology.org/2021.tacl-1.68.pdf)|
|Polish ğŸ‡µğŸ‡±|2020-05|[KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://arxiv.org/abs/2005.00630)|NER, Sentence Relatedness, Textual Entailment, Cyberbullying Detection, Sentiment Analysis (In-Domain & Out-of-Domain), Question Answering, Paraphrase Detection, Sentiment Analysis (Allegro Reviews)|[[paper]](https://arxiv.org/abs/2005.00630)|
|Polish ğŸ‡µğŸ‡±|2022-12|[This is the way: Designing and Compiling LEPISZCZE, a Comprehensive NLP Benchmark for Polish](https://proceedings.neurips.cc/paper_files/paper/2022/file/890b206ebb79e550f3988cb8db936f42-Paper-Datasets_and_Benchmarks.pdf)|Sentiment Analysis, Abusive Clauses Detection, Political Advertising Detection, NLI, NER, POS Tagging, Paraphrase Classification, Punctuation Restoration, Dialogue Acts Classification |[[paper]](https://proceedings.neurips.cc/paper_files/paper/2022/file/890b206ebb79e550f3988cb8db936f42-Paper-Datasets_and_Benchmarks.pdf)|
|Portuguese ğŸ‡µğŸ‡¹ğŸ‡§ğŸ‡·|2024-04|[PORTULAN ExtraGLUE Datasets and Models](https://arxiv.org/abs/2404.05333)|SST-2, MRPC, STS-B, MNLI, QNLI, RTE, WNLI, BoolQ, MultiRC, CoPA |[[paper]](https://arxiv.org/abs/2404.05333)|
|Romanian ğŸ‡·ğŸ‡´|2021-12|[LiRo: Benchmark and Leaderboard for Romanian Language Tasks](https://openreview.net/pdf?id=JH61CD7afTv)|Document Classification, NER, Machine Translation, Sentiment Analysis, POS Tagging, Dependency Parsing, Language Modeling, QA, STS, Gender Debiasing|[[paper]](https://openreview.net/pdf?id=JH61CD7afTv) [[web]](https://lirobenchmark.github.io/)|
|Russian ğŸ‡·ğŸ‡º|2020-10|[RussianSuperGLUE](https://arxiv.org/abs/2010.15925)|Commonsense/COPA-like, RTE/NLI, QA, WSC-like, Paraphrase|[[paper]](https://arxiv.org/abs/2010.15925) [[ACL Anthology]](https://www.aclweb.org/anthology/2020.emnlp-main.381.pdf)|
|Slovak ğŸ‡¸ğŸ‡°|2025-01|[skLEP: A Slovak General Language Understanding Benchmark](https://arxiv.org/abs/2506.21508)|Sentiment Analysis, NER, Text Classification, Paraphrase Detection, Word Sense Disambiguation|[[paper]](https://arxiv.org/abs/2506.21508)|
|Slovenian ğŸ‡¸ğŸ‡®|2022-02|[Slovene SuperGLUE Benchmark: Translation and Evaluation](https://arxiv.org/abs/2202.04994)|BoolQ, CB, COPA, MultiRC, RTE, WSC|[[paper]](https://arxiv.org/abs/2202.04994)|
|Swedish ğŸ‡¸ğŸ‡ª|2023-12|[Superlim: A Swedish Language Understanding Evaluation Benchmark](https://aclanthology.org/2023.emnlp-main.506/)|Absabank-Imm, Argumentation Sentences, DaLAJ-GED, SweParaphrase, SweDN, SweFAQ, SweNLI, SweWiC, SweWinograd, SuperSim, Swedish Analogy, SweSAT, SweDiagnostics, SweWinogender |[[paper]](https://aclanthology.org/2023.emnlp-main.506/)|
|Vietnamese ğŸ‡»ğŸ‡³|2024-06|[ViGLUE: A Vietnamese General Language Understanding Benchmark and Analysis of Vietnamese Language Models](https://aclanthology.org/2024.findings-naacl.261.pdf)|MNLI, QNLI, RTE, VNRTE, WNLI, SST2, VSFC, VSMEC, MRPC, QQP, CoLA, VToC|[[paper]](https://aclanthology.org/2024.findings-naacl.261.pdf) [[code]](https://github.com/trminhnam/ViGLUE) [[data]](https://huggingface.co/datasets/tmnam20/ViGLUE)|

### Knowledge-Based Benchmarks

*These benchmarks focus on factual knowledge, domain expertise, curriculum-based assessments, and subject-matter competency. They test models' ability to recall and apply knowledge from specific academic domains, cultural contexts, or educational curricula.*

|Language|Date|Title|Tasks|Links|
|:---:|:---:|:---:|:---:|:---:|
|Cantonese ğŸ‡­ğŸ‡°ğŸ‡¨ğŸ‡³|2024-08|[How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models](https://arxiv.org/abs/2408.16756)|Yue-TruthfulQA, Yue-GSM8K, Yue-ARC-C, Yue-MMLU, Yue-TRANS|[[paper]](https://arxiv.org/abs/2408.16756)|
|Chinese ğŸ‡¨ğŸ‡³|2024-09|[CJEval: A Benchmark for Assessing Large Language Models Using Chinese Junior High School Exam Data](https://arxiv.org/abs/2409.16202)|Multi-Choice QA, Bool QA, Fill-in-the Blank QA, Analysis QA|[[paper]](https://arxiv.org/abs/2409.16202)|
|Italian ğŸ‡®ğŸ‡¹|2024-06|[The Invalsi Benchmarks: Measuring Linguistic and Mathematical Understanding of Large Language Models in Italian](https://arxiv.org/abs/2406.17535)|Locate and Identify Information, Reconstruct Meaning, Reflect on Content/Form, Word Formation, Lexicon and Semantics, Morphology, Spelling, Syntax, Textuality and Pragmatics, Cloze (Fill-in-the-Blank), Multiple Choice (MC), Multiple Complex Choice (MCC), Unique Response (RU), Short Response (RB)|[[paper]](https://arxiv.org/abs/2406.17535)|
|Korean ğŸ‡°ğŸ‡·|2024-06|[KMMLU: Measuring Massive Multitask Language Understanding in Korean](https://arxiv.org/abs/2402.11548)|Multichoice QA across 45 subjects, including STEM, Humanities, Applied Sciences|[[paper]](https://arxiv.org/abs/2402.11548)|
|Russian ğŸ‡·ğŸ‡º|2024-01|[MERA: A Comprehensive LLM Evaluation in Russian](https://arxiv.org/abs/2401.04531)|MathLogicQA, MultiQ, PARus, RCB, ruModAr, ruMultiAr, ruOpenBookQA, ruTiE, ruWorldTree, RWSD, SimpleAr, BPS, CheGeKa, LCS, ruHumanEval, ruMMLU, USE, ruDetox, ruEthics, ruHateSpeech, ruHHH|[[paper]](https://arxiv.org/abs/2401.04531) [[web]](https://mera.a-ai.ru/en)|
